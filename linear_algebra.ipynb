{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "from sklearn import linear_model\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Review\n",
    "TODO\n",
    "Matrices, transposes, inverses, spans, subspace / column space, matrix multiplication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal Projection\n",
    "## Orthogonal Complements\n",
    "If $W$ is a subspace of $\\R^n$, then its **orthogonal complement** is the subspace:  \n",
    "$W^{\\perp} = \\{\\vec{v} \\in \\R^n| \\vec{v} \\cdot \\vec{w} = 0\\ for\\ all\\ \\vec{w}\\ in\\ W\\}$ \n",
    "- Recall that two vectors are orthogonal, or perpendicular, if their dot product is 0 ( because the dot product can be computed as $|\\vec{v}||\\vec{w}| \\cos \\theta$, where $\\theta$ is the angle between the vectors, and if that equation is set equal to 0, then solving for theta yields $\\theta = \\cos^{-1}0 = 90^{\\degree}$).  \n",
    "- Thus, if we have some subpsace $W$ which consists of the vectors $\\vec{v}$, then the orthogonal complement of this subspace is another subspace, $W^{\\perp}$, where all its vectors $\\vec{w}$ are orthogonal with the vectors in $W$.  \n",
    "\n",
    "We can use this concept to define the orthgonal complement of a column space.  \n",
    "If $A$ is a matrix and $W = Col(A)$, then $W^{\\perp} = Nul(A^T)$ (ie, the null space of A transposed).  \n",
    "Since column spaces are also **spans**, we can rephrase:  \n",
    "- Let $\\vec{v_1}, \\vec{v_2}, ..., \\vec{v_n}$ be vectors in $\\R^n$, and let $W = Span\\{ \\vec{v_1}, \\vec{v_2}, ..., \\vec{v_n} \\}$, then:  \n",
    "- $W^{\\perp}$ = {all vectors orthogonal to each $\\vec{v_1}, \\vec{v_2}, ..., \\vec{v_n}$} = $Nul((\\vec{v_1}, \\vec{v_2}, ..., \\vec{v_n})^T)$  \n",
    "\n",
    "We can use this and solve for the orthogonal complement of a span by solving a system of linear equations, check out examples here: \n",
    "https://textbooks.math.gatech.edu/ila/orthogonal-complements.html  \n",
    "The basic formula is:  \n",
    "1. Find the null-space of the given span using row reduction.    \n",
    "1. Write in parametric form.  \n",
    "1. Use this identify the the orthogonal complement of the span.  \n",
    "An example:  \n",
    "Find all vectors orthogonal to (column) vector $v = (1, 1, -1)^T$.  \n",
    "Create matrix $A$ = ( -- $v$ --) = (1, 1, -1).  \n",
    "Find the null space by row reducing. This matrix is already in RREF.  \n",
    "The solution set is thus $x_1 = -x_2 + x_3$. Writing in parametric form we get $x = x_2 (-1, 1, 0)^T + x_3 (1, 0, 1)^T$.  \n",
    "That gives us $Span((-1, 1, 0)^T, (1, 0, 1)^T)$ which is a plane.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = np.array([1, 7, 2])\n",
    "vec2 = np.array([-2, 3, 1])\n",
    "# to plot the span as a plane, we need a normal vec to the plane\n",
    "normal = np.cross(vec1, vec2)\n",
    "# and we solve for plane eq given that (r - r0) * n = 0 (picl vec1 as r0)\n",
    "# ==> (<x,y,z> - <x0, y0, z0>) . (a, b, c) = 0\n",
    "# ==> a(x-x0) + b(y-y0) + c(z-z0) = 0\n",
    "# ==> ax + by + cz - (a*x0 + b*y0 + x*z0) = 0\n",
    "# ==> ax + by + cz + d = 0\n",
    "# solve for d: d = -(ax + by + cz)\n",
    "d = -vec1.dot(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create x,y\n",
    "xx, yy = np.meshgrid(range(-10, 10), range(-10, 10))\n",
    "\n",
    "# calculate corresponding z at any given x,y (need z = f(x,y))\n",
    "# ax + by + cz + d = 0 ==> z = -(ax + by + d) / c \n",
    "z = (-normal[0] * xx - normal[1] * yy - d) * 1. /normal[2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(xx, yy, z, alpha = 0.5)\n",
    "ax.scatter(vec1[0], vec1[1], vec1[2], color=\"green\", label=\"vec1\")\n",
    "ax.scatter(vec2[0], vec2[1], vec2[2], color=\"blue\", label=\"vec2\")\n",
    "ax.scatter(normal[0]/2, normal[1]/2, normal[2]/2, color = \"red\", label=\"normal\")\n",
    "ax.plot([0, normal[0]/2], [0, normal[1]/2], [0, normal[2]/2], color=\"red\") #rescale to make smaller\n",
    "\n",
    "ax.view_init(0, 45, None)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# vec1 and vec2 are on our plane representation of their column space, and the\n",
    "# orthogonal complement is the red line, which is also the normal vector of the \n",
    "# plane we plotted.\n",
    "# Interactive: https://www.geogebra.org/3d/wd2czpwe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection\n",
    "Let $W$ be a suspace of $\\R^n$ and $x$ be a vector in $\\R^n$, then we denote the closest vector to $x$ on $W$ by $x_W$.  \n",
    "- Saying that $x_W$ is the closest vector to $x$ on $W$ is equivalent to saying that the difference between $x$ and $x_W$ is orthogonal to (the vectors in) $W$. That is, the vector on $W$ that is closest to $x$ is always the vector on $W$ that lies in such a way that $(x - x_W)$ is perpendicular to $W$.    \n",
    "<img src=\"assets/la_closestvec.png\" width=\"300\" height=\"200\">    \n",
    "-  Above we can see vector $x$ outside of subsapce $W$, and its closest vector on $W$ is $x_W$. The difference between these vectors $(x - x_W)$ is itself a vector, and it is orthogonal to $W$. We will come to know this orthogonal vector as the \"residual\" vector.  \n",
    "\n",
    "As we learned above, subspace $W$ has an **orthogonal complement** called $W^{\\perp}$. Thus, $x$ also has a \"closest vector\" on that subspace, $x_{W^{\\perp}}$.  \n",
    "Again, this closest vector is the vector on $W^{\\perp}$ such that the difference between $x$ and $x_{W^{\\perp}}$, $(x - x_{W^{\\perp}})$, is orthogonal to $W^{\\perp}$.      \n",
    "We already know what this closest vector is: $(x - x_W)$. We know this vector is orthogonal to $W$ and is thus in $W^{\\perp}$.   \n",
    "So if $x_{W^{\\perp}} = x - x_W$, then we by rearranging this equation we can decompose $x$ into two components:  \n",
    "$x = x_W + x_{W^{\\perp}}$  \n",
    "- This is the **orthogonal decomposition** of $x$ with respect to $W$.  \n",
    "- The closest vector to $x$ on $W$, $x_W$, is called the **orthogonal projection** of $x$ onto $W$.  \n",
    "- The distance from $x$ to $W$ is the magnitude of $x_{W^{\\perp}}$: $||x_{W^{\\perp}}||$ = $||(x - x_W)||$  \n",
    "\n",
    "For example, if $x$ is <1, 2, 3> and $W$ is the xy-plane (so $W^{\\perp}$ is the z-axis), then $x$ can be \"reached\" by adding \"tail to tip\" the vector $x_W$ which extends from the origin to the point in the xy-plane below x, <1, 2, 0>, with the vector $x_{W^{\\perp}}$ which extends vertically from the xy-plane to $x$, <0, 0, 3>. In other words, we decompose x into a **horizontal** and **vertical** component.  \n",
    "        <img src=\"assets/la_orthog_decomp.png\" width=\"200\" height=\"200\">  \n",
    "\n",
    "How do we compute $x_W$ - the orthogonal projection of $x$ onto $W$? We solve the following system of linear equations, where $A$ is the matrix whose column space is $W$ (ie, $Col(A) = W$) and $c$ is the unkown vector:  \n",
    "$A^TAc = A^Tx$  \n",
    "- This can be solved through row-reduction. Proof in Interactive Linear Algebra:\n",
    "- Since $x_W$ lies in $W = Col(A)$ by definition (ie, $x_W$ is spanned by $Col(A)$), then there is a vector $c$ in $R^n$ for which $Ac = x_W$, since some linear combination of A's column vectors can reach $x_W$. Thus the vector $x - x_W == x - Ac$ lies in $W^{\\perp}$ (it is the \"residual\" vector as above), and $W^{\\perp} = Nul(A^T)$ (as shown in the orthogonal complements section). \n",
    "- In other words, $x-Ac$ is orthogonal to each column $a_j$ of $A$, so that $a_j \\cdot (x-Ac) = 0$, and thus $a_j^T \\cdot (x-Ac) = 0$. Since each $a_j^T$ is a row of $A^T$, then $A^T (x - Ac) = 0$, which, after distributing, can be rewritten as $A^TAc = A^Tx$.    \n",
    "- This means that this is a \"consistent\" system of equations meaning that if $c$ is any solution to the system of equations, then $x_W = Ac$ \n",
    "\n",
    "Side note - orthogonal projection onto a line:  \n",
    "- If we wish to project vector $x$ onto line $L = Span(u)$, we can use our equation $A^TAc = A^Tx$ which becomes $u^Tuc = u^Tx$.  \n",
    "- However, since $u$ is a vector this simplifies to $(u \\cdot u)c = u \\cdot x$ so that $c = \\frac{u \\cdot x}{u \\cdot u}$\n",
    "- As $x_W = Ac$, so does $x_L = u \\cdot c$ and so $x_L =  \\frac{u \\cdot x}{u \\cdot u} u$\n",
    "- You may recognize this formula as the \"vector projection\" of x onto u, sometimes written as $proj_{u}x = \\frac{u \\cdot x}{|u|^2}u$ where $|u|$ is the magnitude of $u$.\n",
    "\n",
    "\n",
    "\n",
    "When solving for $x_W$, we may have some vector $x$ which we want to orthogonally decompose with respect to some subspace, $W$. We want to write $W$ as the column space of a matrix $A$ and then calculate the matrix $A^TA$ and vector $A^Tx$, before plugging these results into $A^TAc = A^Tx$ to solve for $c$ (since, $x_W = Ac$).   \n",
    "- [Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/projections.html) has some great examples of how to solve these kinds of problems.  \n",
    "\n",
    "It turns out that if we start with a **basis** for $W$, so that the columns of our matrix $A$ are **linearly independent**, then $A^TA$ is invertible. This simplifies the calculation of $x_W$, because:  \n",
    "- We can now rewrite $A^TAc = A^Tx$ to solve for $c$, so $c = (A^TA)^{-1}A^Tx$.\n",
    "- Since we know $x_W = Ac$, then we add one more matrix multiplcation by $A$ and we have the following key formula:  \n",
    "\n",
    "**Calculating the Orthogonal Projection of x onto W when A has linearly independent columns**, for W = Col(A):    \n",
    "$x_W = A(A^TA)^{-1}A^Tx$  \n",
    "- This requires that, if $W$ is a subspace of $\\R^n$ with basis $v_1, v_2, ..., v_n$, then $A$ is a matrix whose columns are $v_1, v_2, ..., v_n$.  \n",
    "\n",
    "\n",
    "**Projection / Hat Matrix**  \n",
    "- Recall that $x_W$ is the orthogonal projection of $x$ onto $W$. \n",
    "- Therefore, the matrix $A(A^TA)^{-1}A^T$ does the work of projecting $x$ onto $W$, and is thus referred to as the **projection matrix**.  \n",
    "- As we will soon see, sometimes we write $x_W$ as $\\hat{x}$, leading us to refer to this matrix as the **hat matrix** of **H** since it \"puts the hat\" on the vector we are projecting.  \n",
    "- This matrix is a linear transformation (a composition of linear transformations), and thus we can think of the projection $x_W$ as a function of $x$, ie, $x_W = T(x)$.  \n",
    "\n",
    "Selected Properties of the projection matrix:    \n",
    "- $T(x) = x$ if and only if $x$ is in $W$\n",
    "    - In other words, $x_W = x$ if $x$ is in $W$ - this is intuituve since we are trying to orthogonally project $x$ onto the subspace it is already in.  \n",
    "- $T(x) = 0$ if and only if $x$ is in $W^{\\perp}$  \n",
    "    - In other words, $x_W = 0$ if $x$ is in $W^{\\perp}$ - here, we are essentially trying to project $x$ onto the subspace that is the orthogonal complement to the subspace that contains $x$.  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Example:_   \n",
    "let $W = Span{(1, 0, -1)^T, (1, 1, 0)^T}$ and $x = (1, 2, 3)^T$\n",
    "Then we can write $A^TA = A^Tx$.  \n",
    "$A^TA$ = \n",
    "```\n",
    "[2 1]\n",
    "[1 2]\n",
    "```\n",
    "and $A^Tx$ =  \n",
    "```\n",
    "[-2]\n",
    "[ 3]\n",
    "```\n",
    "Form augmented matrix and row-reduce:  \n",
    "```\n",
    "[2 1 | -2] -RREF--> [1 0 | -7/3] ---> c = [-7/3]\n",
    "[1 2 |  3]          [0 1 |  8/3]          [ 8/3]\n",
    "```\n",
    "Thus, $x_W = Ac$ = [1/3, 8/3, 7/3] = [0.333, 2.666, 2.333]\n",
    "\n",
    "Alternatively we can calculate the projection matrix, which will be easier for larger inputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 2.66666667, 2.33333333])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 1],\n",
    "              [0, 1],\n",
    "              [-1, 0]])\n",
    "x = np.array([1, 2, 3]).T # column vec\n",
    "\n",
    "Proj = A @ np.linalg.inv(A.T @ A) @ A.T\n",
    "x_W = Proj @ x\n",
    "x_W"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Sets\n",
    "Quick facts:  \n",
    "- a set of (nonzero) vectors {$u_1, u_2, ..., u_m$} is **orthogonal** if $u_i \\cdot u_j = 0$ whenever $ i \\not= j$\n",
    "    - that is, any combination of two different vectors in the set is a combination of orthogonal vectors, and thus their dot product is 0.  \n",
    "- the set is **orthonormal** if it is orthogonal and in addition $u_i \\cdot u_i = 1$ for all $i$.   \n",
    "- any orthogonal set can be made orthonormal if each vector is converted to a _unit vector_ (recall, this can be done by dividing a vector by its magnitude).  \n",
    "- since an orthogonal set is linearly independent, it is also the basis for its span.  \n",
    "\n",
    "### Using Orthogonal Sets for Orthogonal Projection\n",
    "Working with orthogonal sets gives us a simple formula for the orthogonal projection of a vector.  \n",
    "Recall that the orthogonal projection of $x$ on a line $L$ spanned by vector $u$ is $x_L =  \\frac{u \\cdot x}{u \\cdot u} u$.  \n",
    "Therefore, if {$u_1, u_2, ..., u_m$} is an orthogonal set of vectors (and thus also the basis for its span, $W$), then the orthogonal projection of $x$ onto $W$ is given by this **projection formula**:  \n",
    "$x_W =  \\frac{x \\cdot u_1}{u_1 \\cdot u_1} u_1 + \\frac{x \\cdot u_2}{u_2 \\cdot u_2} u_2 + ... +  \\frac{x \\cdot u_m}{u_m \\cdot u_m} u_m$.  \n",
    "\n",
    "Since $L_i = Span(u_i)$, this could be written $x_W = x_{L_1} + x_{L_2} + ... + x_{L_m}$ - in other words, for an orthogonal basis the projection of $x$ onto $W$ is the _sum of the projections onto the lines spanned by the basis vectors_.  \n",
    "\n",
    "Recall from above that if $x$ is in $W$ then $x_W = x$. Therefore, if we have a subspace $W$ with an orthogonal basis {$u_1, u_2, ..., u_m$} and $x$ is a vector in $W$, then we can write $x$ as a linear combination of basis vectors:  \n",
    "$x = x_W = \\frac{x \\cdot u_1}{u_1 \\cdot u_1} u_1 + \\frac{x \\cdot u_2}{u_2 \\cdot u_2} u_2 + ... +  \\frac{x \\cdot u_m}{u_m \\cdot u_m} u_m$.  \n",
    "\n",
    "### Orthonormal sets and projection\n",
    "_Linear Algebra and its Applications, pg. 353._  \n",
    "- If {$u_1, ..., u_m$} is an orthonormal basis for a subspace $W$ of $\\R^n$, then the orthogonal projection of $x$ onto $W$ simplifies (since $u_m \\cdot u_m = 1$) to \n",
    "$x_W = (x \\cdot u_1)u_1 + ... + (x \\cdot u_m)u_m$.  \n",
    "- So if $U$ = [$u_1, ..., u_m$], then $x_W = UU^Tx$ for all $x$ in $\\R^n$  \n",
    "    - since the above formula can be re-written so $x_W = (x \\cdot u_1)u_1 + ... + (x \\cdot u_m)u_m = (u_1^Tx)u_1 + ... + (u_m^Tx)u_m$, so that the \"weights\" in the parentheses are the elements of $U^Tx$, and this matrix is multiplied by $U$, confirming that $x_W = UU^Tx$.  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gram-Schmidt Process  \n",
    "The work above demonstrates that orthogonal projections are easier to compute when we have an orthogonal basis for a subspace. Therefore, it would be nice if it were possible to compute an orthogonal basis for any subspace.  \n",
    "Let {$v_1, v_2, ..., v_m$} be a basis for subspace $W$ in $\\R^n$.  \n",
    "We can use the following Gram-Schmidt process:  \n",
    "1. Define $u_1 = v_1$.  \n",
    "1. Define $u_2 = (v_2)_{Span(u_1)^{\\perp}}$ = $v_2 - \\frac{v_2 \\cdot u_1}{u_1 \\cdot u_1} u_1$  \n",
    "    - This basically means \"$u_2$ is equal to the orthogonal projection of $v_2$ onto the orthogonal complement of the span of $u_1$\"  \n",
    "    - In earlier formulas, we would have written $W = Span(u_1)$, its orthogonal complement as $W^{\\perp}$, so that we could have written $u_2 = (v_2)_{W^{\\perp}}$\n",
    "1. Define $u_3 = (v_3)_{Span(u_1, u_2)^{\\perp}}$ = $v_3 - \\frac{v_3 \\cdot u_1}{u_1 \\cdot u_1} u_1 - \\frac{v_3 \\cdot u_2}{u_2 \\cdot u_2} u_2 $  \n",
    "...  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp; m. Define $u_m = (v_m)_{Span(u_1, u_2, ..., u_m)^{\\perp}}$ = $v_m - \\Sigma_{i=1}^{m-1} \\frac{v_m \\cdot u_i}{u_i \\cdot u_i} u_i$  \n",
    "Then {$u_1, u_2, ..., u_m$} is an orthogonal basis for the same subspace $W$.   \n",
    "\n",
    "The proof for this process can be read in [Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/orthogonal-sets.html). Essentially, one can prove that each $u_i$ is in $W$ and nonzero, and that the set of all $u_i$ is an orthogonal set. Therefore, we can use the projection formula to calculate $(v_m)_{Span(u_1, ..., u_{m-1})^{\\perp}}$ - ie, the orthonal projection of $v_m$ onto the orthogonal complement of the span of the set of $u_i$.   \n",
    "- Geometrically, once we have $u_1$, we replace $v_2$ by the part that is orthogonal to \"$W_1$\", where $W_1$ is the span of $u_1$ and hence also the span of $v_1$  \n",
    "- Once we have $u_1$ and $u_2$, we replace $v_3$ with the part that is orthogonal to $W_2 = Span(u_1, u_2) = Span(v_1, v_2)$    \n",
    "<img src=\"assets/la_gramschmidt.png\" width=\"200\" height=\"200\">  \n",
    "- Above, we can see that $u_3$ is equivalent to $(v_3 - (v_3)_{W_2^{\\perp}})$, the residual vector after we project $v_3$ onto $W_2$.  \n",
    "\n",
    "You can use the Gram-Schmidt process to produce an orthogonal basis from any spanning set. If we started with a spanning set that was linearly dependent, so that for some vector in the starting span $u_i = 0$, we simply throw away $u_i$ and continue with the procedure.  \n",
    "\n",
    "[Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/orthogonal-sets.html) suggests that it is worth the time to run the Graham-Schmidt process to produce an orthogonal basis if you have to compute projections of many vectors onto the same subspace.  \n",
    "This process is also related to QR decomposition, which is a common technique used for computing linear regression.  \n",
    "https://cran.r-project.org/web/packages/matlib/vignettes/gramreg.html \n",
    "\n",
    "\n",
    "_Example:_ find an orthogonal basis {u1, u2, u3} for $W$, the subspace spanned by vectors v1, v2, v3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u0 = <1, 1, 0>\n",
      "u1 = <0, 0, 1>\n",
      "u2 = <1, -1, 0>\n",
      "Orthogonal basis?: True\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([1,1,0])\n",
    "v2 = np.array([1,1,1])\n",
    "v3 = np.array([3,1,1])\n",
    "def vector_project(v, u):\n",
    "    # project vector v onto basis vector u\n",
    "    return (np.dot(v, u) / np.dot(u, u)) * u\n",
    "\n",
    "# gram-schmidt process\n",
    "u1 = v1\n",
    "u2 = v2 - vector_project(v2, u1)\n",
    "u3 = v3 - vector_project(v3, u1) - vector_project(v3, u2)\n",
    "\n",
    "for i, u in enumerate([u1, u2, u3]):\n",
    "    print(f\"u{i} = <{', '.join([str(int(n)) for n in u])}>\")\n",
    "# confirm {u1, u2, u3} is an orthogonal basis for W\n",
    "print(\"Orthogonal basis?:\",\n",
    "    (np.dot(u1, u2), np.dot(u1, u3), np.dot(u2, u3)) == (0., 0., 0.)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares  \n",
    "Linear Algebra is broadly concerned with systems of linear equations, \"$Ax = b$\" (see [Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/overview.html) for an overview).  \n",
    "In general, we wish to solve for the (vector of) unkowns, $x$.  \n",
    "In the sciences and social sciences, $x$ might be a vector of parameters that model how some inputs, (the columns of) $A$, are linearly combined to produce an output, $b$.   \n",
    "We can use data to calculate these parameters, but that suggests that we might be working with messy, real-world numbers and trying to model some process that is noisy or that we do not fully understand. Therefore, $Ax = b$ may not have an exact solution.     \n",
    "In this scenario, we can use least-squares methods to find the _best approximate solution_ for the system.  \n",
    "\n",
    "## Least-Squares Solution\n",
    "If $A$ is an $m \\times n$ matrix and $Ax = b$ is an inconsistent matrix equation, then the **least-squares solution**, or _best approximate solution_, to the equation is a vector $\\hat{x}$ in $\\R^n$ such that $dist(b, A\\hat{x}) \\le dist(b, Ax)$ for all other vectors $x$ in $\\R^n$.  \n",
    "- The distance between two vectors, $dist(v, w) = |v - w|$. Intuitively, a vector $v$ minus a vector $w$ yields the vector that points from $w$ to $v$ ($v$ is the terminal point). Thus, the magnitude of this vector is the distance between the two vectors.  \n",
    "- Recall, magnitude of a vector $x = <x_1, x_2, x_3> = \\sqrt(x_1^2 + x_2^2 + x_3^2)$  \n",
    "So a least-squares solution **minimizes the sum of squares of the differences between the entries of $A\\hat{x}$ and b**.  \n",
    "- That is, it solves $Ax = b$ as _closely_ as possible.  \n",
    "    - Above, we use distance to convey that the vector $b$ is closer to $A\\hat{x}$ than to $Ax$ for other $x$.  \n",
    "- \"The sum of squares of the differences\" is also called the **Residual Sum of Squares** - the \"residual vector\" being the vector $v-w$ in the example above.  \n",
    "\n",
    "To do so, least-squares relies on the fact that _closest vector_ of the form $Ax$ to $b$ is the orthogonal projection of $b$ onto $Col(A)$:  \n",
    "- Recall that a matrix equation $Ax = b$ has a solution if and only if $b$ is in the span of the columns of $A$ - this is intuitive because some linear combination of $A$'s columns, where the components of $x$ are the coefficients, must be able to \"reach\" the vector $b$.  \n",
    "- Recall that the column space of $A$ is the set of all $c$ such that $Ax = c$ is consistent - again, this is intuitive: the \"column space\" is the subspace where all vectors in it can be \"reached\" by the columns of $A$.  \n",
    "- This also means that $Col(A)$ is the set of vectors of the form $Ax$.  \n",
    "- Hence, the _closest vector_ of the form $Ax$ to the vector $b$ is the orthogonal projection of $b$ onto $Col(A)$.  \n",
    "    - Recall from **orthogonal projection**, that saying $x_W$ is the _closest vector_ to $x$ on $W$ is equivalent to saying that the difference between $x$ and $x_W$, $(x - x_W)$, is orthogonal to (the vectors in) $W$. Recall $x_W$ is the \"orthogonal projection\" of $x$ onto $W$. \n",
    "    - Similarily, the closest vector to $b$ on $Col(A)$ is the orthogonal projection of $b$ onto $Col(A)$, or $b_{Col(A)}$\n",
    "\n",
    "Thus, a least-suares solution of $Ax = b$ is a solution $\\hat{x}$ of the consistent equation $Ax = b_{Col(A)}$.  \n",
    "- If $Ax = b$ is consistent, then $b_{Col(A)} = b$ so the least-squares solution is the same as the usual solution.  \n",
    "    - (Recall, that the $x_W = x$ if $x$ is in $W$, since this implies we are orthogonally projecting $x$ onto the subspace $W$ that it is already in.)  \n",
    "- If $Ax=b$ is inconsistent, then $\\hat{x}$ is the best approximate solution.  \n",
    "\n",
    "Note that if we have solved for $\\hat{x}$, and $v_1, ..., v_n$ are the columns of $A$, then $A\\hat{x} = \\hat{x}_1 v_1 + ... + \\hat{x}_n v_n = b_{col(A)}$.  \n",
    "- If we define $y = b_{Cola(A)}$ and $\\hat{\\beta} = \\hat{x}$, this looks just like a typical linear regression formula $y = \\beta_1 + \\beta_2 v_2$, where $v_1 = 1$\n",
    "\n",
    "Geometrically:    \n",
    "<img src=\"assets/la_leastsquares_coords.png\" width=\"300\" height=\"200\">  \n",
    "- Let A be an $n \\times 2$ matrix with $n$-dimensional column vectors $v_1, v_2$. Then $v_1$ and $v_2$ span the $Col(A)$, represented by the pink plane. We can think of this column space as a coordinate system, where $v_1$ and $v_2$ are the axes. Thus the coefficients, $x_1$ and $x_2$, are the \"coordinates\" of $b_{Col(A)}$ in this coordinate system. In other words, to reach the orthogonal projection of $b$ onto $Col(A)$, start at the origin. Then travel $x_1$ $units_{v_1}$ ($v_1$ units) in the $v_1$ direction and $x_2$ $units_{v_2}$ ($v_2$ units) in the $x_2$ direction.  \n",
    "    - We can always _think_ of the entries of $\\hat{x}$ as the coordinates of $b_{Col(A)}$. Only if the columns are linearly independent are these actually basis-coordinates.   \n",
    "- In practice, this suggests that we input some row of data from $A$ and our coefficients $\\hat{x}$ rescale them such that they reproduce the orthogonal projection of $b$ onto the column space of $A$, which we know is the closest approximation of $b$ that takes the form $Ax$. \n",
    "    - When we put in new data, we are hoping that the coefficients we found $\\hat{x}$ continue to map $A_{new}$ to $b_{Col(A_{new})}$, which seems likely as long as the training sample is representative of the true population.  \n",
    "\n",
    "Again, Least Squares chooses the $\\hat{x}$ such that the overall Residual ($b - b_{Col(A)}$) Sum of Squares is minimized.  \n",
    "\n",
    "In summary:  \n",
    "- Say we have an inconsistent system of equations $Ax = b$, and that we want to get $Ax$ as close as possible to $b$. Well, the closest $Ax$ can get to $b$ is $b_{Col(A)}$, the orthogonal projection of $b$ onto the column space of $A$. So we're going to pick the vector of coefficients $\\hat{x}$ that contains the coordinates of $b_{Col(A)}$ (coordinates in the basis {$v_1 ... v_n$} where $v_i$ are the columns of $A$).  \n",
    "- This means we need to solve for $\\hat{x}$ using $A$ and $b$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Least-Squares Solution\n",
    "In the orthogonal projection section, we learned that we can solve problems of orthogonal projection - in other words, compute $b_{Col(A)}$ - by solving the following system of equations for the unkown vector $x$:    \n",
    "$A^TAx = A^Tb$  \n",
    "- Compute matrix $A^TA$ and vector $A^Tb$, form augmented matrix, and row-reduce. The solution $\\hat{x}$ is **a** least-squares solution, and $b_{Col(A)} = A\\hat{x}$  \n",
    "- The above matrix equation is called the **normal equations** for $Ax = b$.  \n",
    "\n",
    "Note that a least-squares solution need not be unique:  \n",
    "- If the columns of $A$ are linearly dependent, there are infinitely many least-squares solutions.  \n",
    "    - ie, when row-reducing the augmented matrix given by the normal equations there will be a free variable (or multiple).  \n",
    "- If the columns of $A$ are linearly independent, there is a unique least-squares solution.  \n",
    "\n",
    "If the columns of $A$ are linearly **independent**, then we learned in orthogonal projection that $A^TA$ is invertible and thus we can simply calculate:  \n",
    "$\\hat{x} = (A^TA)^{-1}A^Tb$  \n",
    "- This is similair to calculating the projection or hat matrix, as discussed in orthogonal projection. To get $b_{Col(A)}$, we do $b_{Col(A)} = A\\hat{x} = A(A^TA)^{-1}A^Tb$ - thus the projection matrix, or the linear transformation that projects $b$ onto $Col(A)$, is $A(A^TA)^{-1}A^T$.\n",
    "\n",
    "### Least-Squares Error  \n",
    "- if we use $\\hat{x}$ to produce $A\\hat{x}$ as an approximation of $b$, the distance from $b$ to $A\\hat{x}$ is called the least-squares error.  \n",
    "- Least-squares error = $||b-A\\hat{x}|| = ||b - b_{Col(A)}|| = ||b - \\hat{b}|| = \\sqrt(\\Sigma_{i=1}^{m} (b_i - \\hat{b}_i)^2)$  \n",
    "    - ie, the **Residual Sum of Squares**.   \n",
    "    - Writing in vector notation, $RSS(\\hat{x}) = \\sqrt((b - A\\hat{x})^T(b - A\\hat{x}))$\n",
    "\n",
    "## Calculating the Least-Squares Solution via Calculus  \n",
    "_Elements of Statistical Learning, pg. 45_  \n",
    "If we consider that the goal of Least-Squares is to minimize the Residual Sum of Squares (RSS), then we can express the problem of finding the unkown parameters, $\\hat{x}$, as a constrained optimization problem:  \n",
    "- Let $RSS(x) = \\sqrt( \\Sigma_{i=1}^N (b_i - (Ax)_i)^2 )$. For simplicity, we can instead drop the square root and minimize $\\Sigma_{i=1}^N (b_i - (Ax)_i)^2$  \n",
    "- Differentiating with respect to $x$:  \n",
    "    - $\\frac{\\partial RSS}{\\partial x} = -2A^T (b - Ax)$\n",
    "        - $(b- Ax)^2$ => power rule => $2(b-Ax)$ => chain rule => $-2A(b-Ax)$\n",
    "    - the second derivatve is $\\frac{\\partial^2 RSS}{\\partial x \\partial x^T} = 2A^TA$.\n",
    "        - since this is postive, we know the first derivative is increasing at this critical point, and we have a minimum and not a maximum.\n",
    "\n",
    "We set the first derivate to zero:  \n",
    "- $A^T(b-Ax) = 0 $ (divide by -2 to simplify)  \n",
    "- Distribute: $A^Tb - A^TAx = 0$  \n",
    "- By simply rearranging terms we now have the **normal equations** of the matrix equation $Ax=b$:  \n",
    "$A^TAx = A^Tb$\n",
    "    \n",
    "Now if $A$ has full column rank - which means that each of the columns of the matrix are linearly independent and thus $A$ is invertible:     \n",
    "- we can multiply both sides by the inverse of $A^TA$ to isolate $x$:  \n",
    "$\\hat{x} = (A^TA)^{-1}A^Tb$  \n",
    "\n",
    "This shows that we can calculate the parameters, $\\hat{x}$, which minimize the RSS using a \"closed-form solution\", where we just perform some calculations using our input matrix $A$ and target vector $b$.  \n",
    "- \"Closed-form\" is often used to describe the fact that the Least Squares algorithm finds the solution for the parameters in a set number of steps, rather than an undefined number of steps. A counterexample is gradient descent, where we fit the parameters of a model using an iterative process which we hope converges eventually.  \n",
    "    - some more reading: https://stats.stackexchange.com/questions/23128/solving-for-regression-parameters-in-closed-form-vs-gradient-descent\n",
    "- Framing least-squares as an optimization problem makes it easier to compare it to other model-fitting algorithms, many of which do not have a closed-form solution. For example, logistic regression does not have a closed-form solution and thus its coefficients must be found using techniques like Newton's method or gradient descent.\n",
    "\n",
    "\n",
    "Example:  \n",
    "Given matrix A and vector b below, find the least-squares solutions of $Ax = b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.T @ A =\n",
      " [[5 3]\n",
      " [3 3]]\n",
      "A.T @ b =\n",
      " [0 6]\n",
      "\n",
      "[5 3 | 0] ==RREF=> [1 0 | -3]\n",
      "[3 3 | 6]          [0 1 |  5]\n",
      "Least-squares solution:\n",
      "x_hat = [-3 5]\n",
      "\n",
      "Orthogonal projection of b onto column space of A: \n",
      "b_Col(A) = A @ x_hat =  [ 5  2 -1]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0, 1],\n",
    "              [1, 1],\n",
    "              [2, 1]])\n",
    "b = np.array([6, 0, 0])\n",
    "\n",
    "ATA = A.T @ A\n",
    "ATb = A.T @ b\n",
    "print(\"A.T @ A =\\n\", ATA)\n",
    "print(\"A.T @ b =\\n\", ATb)\n",
    "\n",
    "print(\"\"\"\n",
    "[5 3 | 0] ==RREF=> [1 0 | -3]\n",
    "[3 3 | 6]          [0 1 |  5]\n",
    "Least-squares solution:\n",
    "x_hat = [-3 5]\n",
    "\"\"\")\n",
    "x_hat = np.array([-3, 5])\n",
    "b_ColA = A @ x_hat\n",
    "print(\"Orthogonal projection of b onto column space of A: \\nb_Col(A) = A @ x_hat = \", b_ColA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Factorization & Least-Squares\n",
    "In some cases, the normal equations $A^TAx = A^Tb$ of $Ax=b$ can be \"ill-conditioned\", meaning that small errors in calculations of $A^TA$ can cause relatively large errors in the solution $\\hat{x}$. If the columns of $A$ are linearly independent, the least-squares solution can be computed more reliably through a QR factorization of $A$.  \n",
    "\n",
    "### QR Factorization \n",
    "The QR factorization of a matrix is related to the Gram-Schmidt process. Recall that we introduced the Gram-Schmidt process as a method for computing an orthogonal basis for a subspace - for example, the subpsace spanned by the columns of $A$.  \n",
    "If an $m \\times n$ matrix $A$ already has linearly independent columns $x_1,...,x_n$, then applying the Gram-Schmidt process to $x_1,...,x_n$ amounts to factoring $A$.  \n",
    "Specifically, $A$ can be factored as $A = QR$ where $Q$ is an $m \\times n$ matrix whose columns form an _orthonormal_ basis for $Col(A)$ and $R$ is an $n \\times n$ upper triangle invertible matrix with positive entries on its diagonal.  \n",
    "- The columns of $A$ form a basis {$x_1,...,x_n$} for $Col(A)$. Construct an orthonormal basis {$u_1,...,u_n$} for $W = Col(A)$ such that Span{$u_1,...,u_n$} = Span{$x_1,...,x_n$}, using the Gram-Schmidt process (which produces an orthogonal basis that can be normalized to be orthonormal) or some other method.  \n",
    "- Then Q = [$u_1,...,u_n$]\n",
    "- For $k = 1,...,n$, $x_k$ is in Span{$x_1,...,x_k$} = Span{$u_1,...,u_k$}, meaning there are constants $r_{1k},...,r_{kk}$ such that $x_k = r_{1k}u_1 + ... + r_{kk}u_k + 0 \\cdot u_{k+1} + ... + 0 \\cdot u_n$\n",
    "    - In other words, each column of $A$, $x_k$, is in the span of the set that includes itself and all $k$ columns before it. This is also true for its orthonormal counterpart, $u_k$. That means that we can write $x_k$ as a linear combination of some constants $r_{kk}$ and the spanning set of orthonormal vectors $u_1,...,u_k$. The vectors outside of that set, $u_{k+1},...,u_n$ do not contribute to $x_k$ and thus are multiplied by scalar 0.\n",
    "    - Assume $r_{kk} \\ge 0$, since if $r_{kk} < 0$, we can simply multiply $r_{kk}$ and $u_k$ by -1.\n",
    "- Then, $x_k$ is a linear combination of of the columns of $Q$ using as weights the entries in the (column) vector $r_k$ = [$r_{1k}, r_{2k}, ..., r_{kk}, 0, ... 0$]  \n",
    "- That is, $x_k = Qr_k$ for k = 1, ... n.\n",
    "- Then R = [$r_1,...,r_n$].\n",
    "    - $R$ is invertible since the columns of $A$ are linearly independent, and is clearly upper triangle (consider the 0s in each $r_k$) with positive entries on its diagonal (since we assumed $r_{kk} \\ge 0$ and its diagonal entries are not zero.)\n",
    "\n",
    "Thus, A = [$x_1,...,x_n$] = [$Qr_1,...,Qr_n$] = $QR$\n",
    "- To find $R$, note that $Q^TQ = I$ because the columns of $Q$ are orthonormal. Thus, $Q^TA = Q^T(QR) = IR = R$, so we can solve for $R$ by computing $Q^TA$.  \n",
    "- Example on pg. 360 of Linear Algebra and Its Applications.  \n",
    "\n",
    "### Using QR for Least-Squares\n",
    "Let $A$ is an $m \\times n$ matrix with linearly independent columns.  \n",
    "Let $A = QR$ be a QR factorization of A. Then for each $b$ in $\\R^m$ the equation $Ax = b$ has a unique solution given by $\\hat{x} = R^{-1} Q^T b$\n",
    "- Proof: given $\\hat{x} = R^{-1} Q^T b$, then $A\\hat{x} = QR\\hat{x} = QR R^{-1} Q^T b = QQ^Tb$. From the definition/construction of QR factorization, we know the columns of $Q$ form an orthonormal basis for $Col(A)$. Recall from the \"orthogonal sets\" section that, the projection of a vector $x$ onto the subpsace $W$ spanned by an orthonormal basis $U$ = [$u_1,...,u_m$] is given by $x_W = UU^Tx$. Therefore, given that $Q$'s columns form an orthonormal basis, then $QQ^Tb$ is the orthogonal projection of $b$ onto $Col(A)$.  \n",
    "- Note: Since $R$ is upper triangular, $\\hat{x}$ can be calculated as the exact solution of the system of equations $Rx = Q^Tb$, where one computes the vector $Q^Tb$ and uses row-operations or back substitution to solve for $x$. This is much faster than computing $R^{-1}$ to solve the original equation.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_project(v, u):\n",
    "    # project vector v onto basis vector u\n",
    "    return (np.dot(v, u) / np.dot(u, u)) * u\n",
    "\n",
    "def gram_schmidt(vectors):\n",
    "    is_list = False\n",
    "    if isinstance(vectors, list):\n",
    "        A = np.column_stack(vectors)\n",
    "        is_list = True\n",
    "    elif isinstance(vectors, np.ndarray):\n",
    "        A = vectors\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    U = np.empty_like(A, dtype=np.float32) #orthogonal basis\n",
    "    _, n_cols = A.shape\n",
    "    # g-s procedure\n",
    "    for i in range(0, n_cols):\n",
    "        U[:, i] = A[:, i].copy()\n",
    "        for j in range(0, i):\n",
    "            U[:,i] -= vector_project(A[:, i], U[:, j])\n",
    "    \n",
    "    if is_list:\n",
    "        U = [U[:, i] for i in range(n_cols)]\n",
    "\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRAM-SCHMIDT: compute orthogonal basis\n",
      "u_0 = <1.0, 1.0, 1.0, 1.0>\n",
      "u_1 = <1.0, -1.0, -1.0, 1.0>\n",
      "u_2 = <1.0, -1.0, 1.0, -1.0>\n",
      "\n",
      "NORMALIZE: compute orthonormal basis\n",
      "u_0 = <0.5, 0.5, 0.5, 0.5>\n",
      "u_1 = <0.5, -0.5, -0.5, 0.5>\n",
      "u_2 = <0.5, -0.5, 0.5, -0.5>\n",
      "\n",
      "Q = \n",
      " [[ 0.5  0.5  0.5]\n",
      " [ 0.5 -0.5 -0.5]\n",
      " [ 0.5 -0.5  0.5]\n",
      " [ 0.5  0.5 -0.5]]\n",
      "R = \n",
      " [[2. 4. 5.]\n",
      " [0. 2. 3.]\n",
      " [0. 0. 2.]]\n",
      "\n",
      "x_hat = <10.0, -6.0, 2.0>\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 3, 5],\n",
    "              [1, 1, 0],\n",
    "              [1, 1, 2],\n",
    "              [1, 3, 3]], dtype=np.float64)\n",
    "b = np.array([3,5,7,-3], dtype=np.float64)\n",
    "\n",
    "# solve for Q via gram-schmidt\n",
    "def magnitude(v):\n",
    "    return np.sqrt(np.sum([(i**2) for i in v]))\n",
    "\n",
    "# use the gram-schmidt process to get an orthogonal basis\n",
    "print(\"GRAM-SCHMIDT: compute orthogonal basis\")\n",
    "U = gram_schmidt(A)\n",
    "for i, u in enumerate([U[:,0], U[:,1], U[:,2]]):\n",
    "    print(f\"u_{i} = <{', '.join(str(e) for e in u)}>\")\n",
    "\n",
    "\n",
    "# normalize to get an orthonormal basis\n",
    "print(\"\\nNORMALIZE: compute orthonormal basis\")\n",
    "U[:,0] /= magnitude(U[:,0])\n",
    "U[:,1] /= magnitude(U[:,1])\n",
    "U[:,2] /= magnitude(U[:,2])\n",
    "for i, u in enumerate([U[:,0], U[:,1], U[:,2]]):\n",
    "    print(f\"u_{i} = <{', '.join(str(e) for e in u)}>\")\n",
    "\n",
    "Q = U\n",
    "print(\"\\nQ = \\n\", Q)\n",
    "\n",
    "# compute R\n",
    "R = Q.T @ A\n",
    "print(\"R = \\n\", R)\n",
    "\n",
    "# compute x_hat (the slow way)\n",
    "x_hat = np.linalg.inv(R) @ Q.T @ b\n",
    "print(f\"\\nx_hat = <{', '.join([str(e) for e in x_hat])}>\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Line of Best-Fit\n",
    "\"Linear regression\" uses least-squares to find a \"line of best fit\" for real data.  \n",
    "Suppose we have measured some data and have found points (0,6), (1,0), (2,0).  \n",
    "If we want to model these data, we have to make some assumptions - for example, lets say that it is reasonable to assume that these points should lie on a line, so that our model takes the form $y = \\beta_0 + \\beta_1x$ and we wish to solve for coefficients $\\beta$.  \n",
    "Well, clearly the points do not lie on a line, but we can use least-squares to find the best approximate solution.  \n",
    "If they did lie on a line, then all of these equation would be satisfied:  \n",
    "$6 = \\beta_0 + \\beta_1 \\cdot 0$  \n",
    "$0 = \\beta_0 + \\beta_1 \\cdot 1$  \n",
    "$0 = \\beta_0 + \\beta_1 \\cdot 2$  \n",
    "We can compose this system of equations in the form $Ax = b$, although we will use notation more common in modeling, $X\\beta = y$:  \n",
    "```\n",
    "X = [1 0]   beta = [b0] y = [6]\n",
    "    [1 1]          [b1]     [0]\n",
    "    [1 2]                   [0]\n",
    "```\n",
    "Calculating the Least-Squares solution using $X^TX\\beta = X^Ty$ (shown below) we will find $\\hat{\\beta} = <-3, 5>$.  \n",
    "Thus, our \"best-fit\" line is $y = -3x + 5$\n",
    "\n",
    "For each _observed_ data point ($x_j, y_j$), there is a corresponding _predicted_ point on the line of best-fit, ($x_j, \\beta_0 + \\beta_1 x_j$). The difference between the oberved and predicted y value is the residual.  \n",
    "The sum of the squares of these residuals is equivalent to the sum of the squares of the distances between the vectors $X\\beta$ and $y$ - which is precisely the quantity that the least-squares process minimizes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.  5.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEpCAYAAACjn4aHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwtUlEQVR4nO3de1wVdf748dfhcBc4iIJIHu9KXvIuiGZqXrDUcjMzv7pqP8tysWStNt3tm7luUt9NbXczM3dDd7toq2mb5oUUxLt5ofASqaFoomAmiCIo5/P7Y+SsyF2YMwd4Px+PefiYOZ+ZeY/H83ZmPjOft0kppRBCCJ25GB2AEKJukGQjhHAISTZCCIeQZCOEcAhJNkIIh5BkI4RwCEk2QgiHkGQjhHAISTZCCIeQZCOEcAhJNqJE586dY/z48YSGhuLr64u/vz9hYWEsX76cir7hkpeXxyuvvEJISAheXl6Eh4cTFxenc+R37/XXX8dkMhWbPD09K7yNXbt2cf/99+Pt7U1wcDAvvPACOTk5OkZdc7gaHYBwThcvXuTs2bM8/vjjNG3alBs3bhAXF8ekSZNISUlh3rx55W5j0qRJrFq1iujoaNq0acOyZct4+OGHiY+P5/7773fAUdydxYsX4+PjY583m80VWi8pKYmBAwfSrl07FixYwNmzZ3n77bc5fvw4GzZs0CvcmkMJUQnDhw9X9erVUzdv3iyz3d69exWg/vznP9uX5ebmqlatWqmIiAjd4ktNTVWAio+Pr/S6s2fPVoDKzMy8q30/9NBDqnHjxiorK8u+bOnSpQpQmzZtuqtt1iZyGWWA+Ph4TCYTa9asKfbZJ598gslkYvfu3QZEVr7mzZtz7do18vPzy2y3atUqzGYzU6ZMsS/z9PRk8uTJ7N69mzNnzgAQGxuLyWTiww8/LLL+vHnzMJlMfPXVV9V/EOVQSpGdnV3hy0WA7Oxs4uLiGD9+PH5+fvblEyZMwMfHh88++0yPUGsUSTYG6N+/P1arlY8//rjYZx9//DGtWrUiIiKi1PVtNhsXL16s0HTjxo0qxZqbm8vFixc5deoUy5cvJzY2loiICLy8vMpc79ChQ7Rt27bIDw8gLCwM0C45AJ566imGDx/OjBkz7AkoOTmZOXPmMHnyZB5++OEqxX83WrZsicViwdfXl/Hjx3PhwoVy10lOTubmzZv06NGjyHJ3d3e6dOnCoUOH9Aq3xpBkYwCTycT48eNZt24dWVlZ9uWZmZls3ryZ8ePHl7l+WloagYGBFZp27txZpVj/8pe/EBgYSIsWLZg0aRK9evVixYoV5a6Xnp5O48aNiy0vXHbu3Dn7sqVLl2I2m5k8eTL5+flMnDiR4OBgFixYUKXYK6t+/fpMmzaNJUuWsGrVKp5++mlWrlxJ3759yc7OLnPd9PR0gFKP+fbjravkBrFBJkyYQExMDKtWrWLy5MkArFy5kps3b5abbIKDgyvcq9O5c+cqxTl27Fh69OhBZmYm69at48KFC+Tm5pa7Xm5uLh4eHsWWF/bs3L6N4OBgFi1axNixY+nbty9JSUnExcUVOysqSU5ODtevX7fP//LLLwBkZWVx8eJF+3I3NzcsFkuZ25o+fXqR+VGjRhEWFsa4ceN47733mDlzZqnrFh5Pacdckb+zWs/om0Z1Wc+ePdWAAQPs87169VK9evVyaAzp6elFpmvXrpXZ/plnnlFWq7Xcdh06dFAPPvhgseVHjhxRgHr//feLfTZs2DAFqClTplQ4/okTJyqg3Klfv34V3uadgoOD1cCBA8ts8+9//1sBKjExsdhno0ePVsHBwXe9/9pCzmwMNGHCBKZPn87Zs2fJy8tjz549vPvuu+WuV1BQQGZmZoX2ERAQgLu7e6mf33naHxsby6RJk0pt//jjj7N06VISExOJjIwsc7s//fRTseWFlxshISFFlv/888/s378fgKNHj2Kz2XBxKf8q/3e/+12RM8ELFy4wfvx43n777SJndfXr1y93W6WxWq1cunSpzDaFf4+Fx3e79PT0YsdbF0myMdCTTz7JjBkz+PTTT8nNzcXNzY0xY8aUu96ZM2do0aJFhfYRHx9P//79S/38zsuxDh06lLm9wsuB2+81laRLly7Ex8eTnZ1d5HJo79699s9vFxUVxZUrV4iJiWHWrFm88847zJgxo8x9ALRv35727dvb50+dOgVA9+7dyzzuilJKcerUKbp27Vpmu44dO+Lq6sr+/ft54okn7Mvz8/NJSkoqsqzOMvrUqq575JFHVKdOnVTbtm3ViBEjKrRObm6uiouLq9B06dKlu4orIyOjxOUjRoxQJpNJHT9+3L4sMzNTHTt2TF29etW+bM+ePcWes7l+/bpq3bq1Cg8PL7LNwkuQv/71r0oppZ588knl5eWlUlJSKh13VZ6zKemYFy1apAC1YMGCIsuPHTumTp8+XWTZ0KFDVePGjVV2drZ92d///ncFqA0bNlQ6ntpGko3BVq1aZb+vsHLlSqPDsZs+fbrq0aOHevXVV9UHH3yg3nzzTdWzZ08FqOeff75I28KH4e78gY8ePVq5urqql19+WS1ZskT17t1bubq6qm3bttnbXLhwQTVs2FANGDBA2Ww2pZRSFy9eVI0aNVIRERGqoKCgUnFXJdl4eXmpSZMmqfnz56tFixapsWPHKpPJpLp06VIkkSqlSrwPdODAAeXh4aG6du2qFi9erP7whz8oT09PNWTIkErHUhtJsjFYXl6eql+/vrJYLCo3N9focOw2b96shg8frkJCQpSbm5vy9fVVffr0UbGxsfakUKi0ZJObm6teeuklFRwcrDw8PFTPnj3Vxo0bi7R57LHHlK+vrzp16lSR5V988YUC1FtvvVWpuKuSbJ5++mnVvn175evrq9zc3FTr1q3VK6+8UuRMpVBJyUYppbZv36569+6tPD09VWBgoIqKiipx/brIpJTUjTLSzZs3CQkJYcSIEfzjH/8wOhwhdCMP9Rls7dq1ZGZmMmHCBKNDEUJXcmZjkL179/Ldd98xd+5cGjZsyMGDB40OSQhdyZmNQRYvXszUqVMJCgrin//8p9HhCKE7ObMRQjiEnNkIIRxCko0QwiGc+nUFm83GuXPn8PX1xWQyGR2OEOIOSimuXLlCSEhIue+yOXWyOXfuHFar1egwhBDlOHPmDE2aNCmzjVMnG19fX0A7kIqMbSKEcKzs7GysVqv9t1oWp042hZdOfn5+kmyEcGIVuc0hN4iFEA4hyUYI4RCSbIQQDqH7PZuffvqJV155hQ0bNnDt2jVat25NbGxssZIXVVFgU+xLvUTGlesE+XoS1iIAs4t0lVc3m81Wbr0oUbu4ublVuCJoeXRNNr/88gt9+vRhwIABbNiwgcDAQI4fP16l8WDvtPFwOnO+PEp61n9H2G9s8WT2iPYM7Vi8rIa4O/n5+aSmpmKz2YwORTiYv78/wcHBVX7WTddk89Zbb2G1WomNjbUvq+jYuRWx8XA6Uz86yJ0vd53Pus7Ujw6yeHw3STjVQClFeno6ZrMZq9VaoYHIRc2nlOLatWtkZGQAJdfEqgxdk81//vMfIiMjGT16NNu2beOee+7hN7/5Dc8880yVt11gU8z58mixRAPaGJsmYM6XRxncPlguqaro5s2bXLt2jZCQELy9vY0ORzhQYeXTjIwMgoKCqnRJpet/UT/++COLFy+mTZs2bNq0ialTp/LCCy+wfPnyEtvn5eWRnZ1dZCrNvtRLRS6d7qSA9Kzr7EstuwSHKF9BQQFAmSVhRO1V+B9MVUs563pmY7PZ6NGjB/PmzQOga9euHD58mPfff5+JEycWax8TE8OcOXMqtO2MK6UnmrtpJ8on76fVTdX1vet6ZtO4ceMiNX0A2rVrR1paWontZ82aRVZWln0qLDRfkiBfzwrFUNF2Qgh96Zps+vTpQ0pKSpFlP/zwA82aNSuxvYeHh/3VhPJeUQhrEUBjiyel5VwTWq9UWIuAu4xe1HT9+/cnOjra6DAAbazp1q1bYzabiY6OZtmyZfj7+1d6O5MmTWLkyJH2eWc6xvLommx++9vfsmfPHubNm8eJEyf45JNP+OCDD4iKiqryts0uJmaP0M6a7kw4hfOzR7SXm8NCNwkJCZhMJi5fvlxu22effZbHH3+cM2fOMHfuXMaMGcMPP/xg//z1118vViW0Ij7//HPmzp1b6fWMoGuy6dmzJ2vWrOHTTz+lY8eOzJ07l3feeYdx48ZVy/aHdmzM4vHdCLYUvVQKtnhKt7dwGjk5OWRkZBAZGUlISAi+vr54eXkRFBRU5W0HBARU6I1rp2BYxaoKyMrKUoDKysoqs93NApvadeKiWnvorNp14qK6WWArs72onNzcXHX06FGnKqJXEf369VNRUVEqKipK+fn5qQYNGqhXX321SJG969evqxdffFGFhIQob29vFRYWVqTA3alTp9Tw4cOVv7+/8vb2Vu3bt1fr16+3F8O7fZo4cWKxGOLj44u1i4+PV7GxscpisSillIqNjS3WJjY2tsRjmjhxonr00UeLHOP06dPt882aNVNvvPGGeuqpp5SPj4+yWq1qyZIlRbaRlpamRo8erSwWi6pfv7565JFHVGpqaql/j2V9/xX9jSqllFMPMVFRZhcTEa0aGB1G3aEUFFwzZt9mb6hE78jy5cuZPHky+/btY//+/UyZMoWmTZvan/WaNm0aR48eZcWKFYSEhLBmzRqGDh1KcnIybdq0ISoqivz8fBITE6lXrx5Hjx7Fx8cHq9XK6tWrGTVqFCkpKfj5+dmfSbld7969SUlJITQ0lNWrV9O7d28CAgI4deqUvc2YMWM4fPgwGzdu5OuvvwbAYrHc9V/R/PnzmTt3Lr///e9ZtWoVU6dOpV+/foSGhnLjxg0iIyOJiIhg+/btuLq68qc//YmhQ4fy3Xff6fp4Q61INsLBCq7BZz7G7PuJHHCtV+HmVquVhQsXYjKZCA0NJTk5mYULF/LMM8+QlpZGbGwsaWlphISEAPDSSy+xceNGYmNjmTdvHmlpaYwaNYr77rsPgJYtW9q3HRCgdT4EBQWVerPX3d3dfrkUEBBAcHBwsTZeXl74+Pjg6upa4ueV9fDDD/Ob3/wGgFdeeYWFCxcSHx9PaGgoK1euxGaz8fe//93epR0bG4u/vz8JCQkMGTKkyvsvjSQbUav16tWryHMiERERzJ8/n4KCApKTkykoKKBt27ZF1snLy6NBA+1M+YUXXmDq1Kls3ryZQYMGMWrUKDp16uTQY6is2+MzmUwEBwfbXzn49ttvOXHiRLH7PNevX+fkyZO6xiXJRlSe2Vs7wzBq39UkJycHs9nMgQMHij2G7+Ojnbk9/fTTREZGsn79ejZv3kxMTAzz58/n+eefr7Y4qpubm1uReZPJZH+BNicnh+7du/Pxxx8XWy8wMFDXuCTZiMozmSp1KWOkvXv3Fpnfs2cPbdq0wWw207VrVwoKCsjIyKBv376lbsNqtfLcc8/x3HPPMWvWLJYuXcrzzz9vv79R+DpHVbi7u1fLdsrTrVs3Vq5cSVBQkMOH2pXXd0WtlpaWxowZM0hJSeHTTz/lb3/7G9OnTwegbdu2jBs3jgkTJvD555+TmprKvn37iImJYf369QBER0ezadMmUlNTOXjwIPHx8bRr1w6AZs2aYTKZWLduHZmZmeTk3P3ZXvPmzUlNTSUpKYmLFy+Sl5dX9YMvwbhx42jYsCGPPvoo27dvJzU1lYSEBF544QXOnj2ryz4LSbIRtdqECRPIzc0lLCyMqKgopk+fzpQpU+yfx8bGMmHCBF588UVCQ0MZOXIk33zzDU2bNgW0s5aoqCjatWvH0KFDadu2Le+99x4A99xzD3PmzGHmzJk0atSIadOm3XWco0aNYujQoQwYMIDAwEA+/fTTqh14Kby9vUlMTKRp06Y89thjtGvXjsmTJ3P9+nXdz3ScutZ3dnY2FouFrKwsqa5goOvXr5OamkqLFi3w9JR3zeqasr7/yvxG5cxGCOEQkmyEEA4hyUYI4RCSbIQQDiHJRgjhEJJsRIU5ccel0FF1le+RJ4hFudzc3DCZTGRmZhIYGChjEdcRSiny8/PJzMzExcWlym+ES7IR5TKbzTRp0oSzZ88WGRpB1A3e3t40bdq0yvXCJNmICvHx8aFNmzZVLuchahaz2Yyrq2u1nM1KshEVZjabq63us6h75AaxEMIhdE02r7/+OiaTqch077336rlLIYST0v0yqkOHDvZxVQFcXeXKTYi6SPdffnWNqyqEqNl0v2dz/PhxQkJCaNmyJePGjSu19G6VnfwHXDunz7aFEFWma7IJDw9n2bJlbNy4kcWLF5Oamkrfvn25cuVKie3z8vLIzs4uMlXI6ZWw92nYHA6/fFuNRyCEqC4OHTzr8uXLNGvWjAULFjB58uRin7/++uvMmTOn2PJyB+bJ+REShkH299rYuH1WwD3DqzN0IUQJnHbwLH9/f9q2bcuJEydK/HzWrFlkZWXZpzNnzlRswz4tYchuaDQQbl6FxEfh+79oxdSEEE7BockmJyeHkydP0rhxyTW4PTw88PPzKzJVmLs/DNgArZ4GZYOD0bB/GthuVkvsQoiq0TXZvPTSS2zbto1Tp06xa9cufvWrX2E2mxk7dqw+O3Rxg7APoOufARMcfw+2jYAbFbz3I4TQja7J5uzZs4wdO5bQ0FCeeOIJGjRowJ49e/QthmUyQbuXoO9qMHtB+kbY3AeuntZvn0KIctXu6go/74fERyA3HTwbwQNfQMPw6g9UiDrKaW8QO1yDHjBkL/h3gusXYEt/SFtldFRC1Em1O9kA1LPC4B0QMgwKrsOO0XAkRnqqhHCw2p9sANx8tUuoUK3sKt/+HvZOhoJ8Y+MSog6pG8kGwMUM3d+BHu+CyQV+jIX4SMi7ZHRkQtQJdSfZFGobBf3WgasvZCTA5gjIPm50VELUenUv2QCEPARDdoJ3U7jyA2zuBRnbjY5KiFqtbiYbAP/7IHIvBPSE/EuwdSCk/svoqISotepusgHwCoZBCWB9HGw3YPcE+O416akSQgd1O9kAuHrD/Suh/Uxt/vBc2PU/Wje5EKLaSLIBrXeqSwyEfwgmVzi9ArY8CNczjI5MiFpDks3tWj0FD24G9/pwcTdsCoeso0ZHJUStIMnmTo0GaGPj+LSCq6e0rvH0OKOjEqLGk2RTEr9QGLIHAvtqw1MkPATHlxgdlRA1miSb0ng2hAfjoPl4UAXwzXNw8EWwFRgdmRA1kiSbspg9IOKfcN8ftfnvF8COUdrQo0KISpFkUx6TCe77X+j9Kbh4wNkvIK4vXPvJ6MiEqFEk2VRU8ydhYDx4BMIvh7SeqkuHjI5KiBpDkk1lBEZorzj4tYPcn+DrvnD2S6OjEqJGkGRTWT4tYMguCB50W9mYhfKKgxDlcFiyefPNNzGZTERHRztql/px94f+X0HrKYCCgzNgf5SUjRGiDA5JNt988w1LliyhU6dOjtidY7i4Qc/3oet8tLIxi7WqnPlZRkcmhFPSPdnk5OQwbtw4li5dSv369fXenWOZTNBuBvT9HMzecH4zxPWBnFNGRyaE09E92URFRTFs2DAGDRqk966MYx0Jg7eDVwhkHYHN4XBxj9FRCeFUXPXc+IoVKzh48CDffPNNhdrn5eWRl5dnn8/OrkGVLAO6aT1VCcPh8rewZQD0Wg7NnjA6MiGcgm5nNmfOnGH69Ol8/PHHeHp6VmidmJgYLBaLfbJarXqFpw/vJrfKxgzXxsPZOQaOzJOeKiHQsSLm2rVr7bW9CxUUFGAymXBxcSEvL6/IZ1DymY3Var37iphGsRXAoZcg5R1tvsVErQa52d3QsISobpWpiKnbZdTAgQNJTk4usuypp57i3nvv5ZVXXimWaAA8PDzw8PDQKyTHcTFD94Xg1xb2Pw+py7XhKvquBo8GRkcnhCF0Sza+vr507NixyLJ69erRoEGDYstrrTZToV4L2PEEZGzTxsbptx782hgdmRAOJ08Q6y1kqPbEsXdTuHJcKxtzYZvRUQnhcLrds6kOlbkedHq557VXG37epz0QGLYUWk40OiohqqQyv1E5s3EUr2AYmABNR2tlY/ZMgm9fBWUzODAhHEOSjSO5ekGfFdDh99r8kTdg51i4mWtsXEI4gCQbRzO5QOc3oFesdjmV9pn2AGDuBaMjE0JXkmyM0nISDLhVNubnvdorDpePGB2VELqRZGOkRv21Kg4+reHqaYjrDec2GR2VELqQZGM0v7YQuQeCHtDKxmwbpg1XIUQtI8nGGXg00C6pWky4VTbmN3BghpSNEbWKJBtnYfaAXsug05+0+ZSFsP1XcCPH0LCEqC6SbJyJyQQd/6B1j7t4wE9faoOqXztrdGRCVJkkG2fUbMxtZWOSbpWNOWh0VEJUiSQbZ1VYNsbSHnLPaYXxzn5hdFRC3DVJNs7MpwUM3gXBg6HgGiT+Co4tkMG4RI0kycbZuVug/3po/Syg4NCL8M1U7f0qIWoQSTY1gYsb9FwM3RYAJjixRMrGiBpHkk1NYTLBvb+FB9beKhsTpz1xnJNqdGRCVIgkm5qmySO3lY05qvVUZe42OiohyiXJpiYK6AaR+6B+V8jL1N4aP73S6KiEKJMkm5rK+x4YlAj3PAK2PNj5JBz+k/RUCaclyaYmc/PRSv/eO0Ob/+5/YfdEKMgrez0hDKBrslm8eDGdOnXCz88PPz8/IiIi2LBhg567rHtczNBtvtZbZTLDqX/B1sGQ97PRkQlRhK7JpkmTJrz55pscOHCA/fv38+CDD/Loo49y5IgMElXt2jwH/b8CNz/I3A6bekF2itFRCWHn8OoKAQEB/PnPf2by5Mnltq1V1RUc5fIRbUycq6e1UQD7fq4N0iWEDpyyukJBQQErVqzg6tWrREREOGq3dY9/BxiyFxr0gvxfIH4InIw1Oioh9KuIWSg5OZmIiAiuX7+Oj48Pa9asoX379iW2LanWt7gLXo1g4FbY8xSkrYS9/08rkNf5T9qA60IYQPd/eaGhoSQlJbF3716mTp3KxIkTOXr0aIltY2JisFgs9slqteodXu3l6gV9PoEOr2rzR2NgxxgpGyMM4/B7NoMGDaJVq1YsWbKk2GclndlYrVa5Z1NVP/4T9j2tvbzZIAwe+EIrmidEFTnlPZtCNputSEK5nYeHh72bvHAS1aDlBHjwa3AP0Mr/bgqHy4eNjkrUMbomm1mzZpGYmMipU6dITk5m1qxZJCQkMG7cOD13K0oS9IBWNsa3DVxLg8294dxGo6MSdYiuySYjI4MJEyYQGhrKwIED+eabb9i0aRODBw/Wc7eiNH5ttIQT1A9uXtG6yH94z+ioRB3h8Hs2lSHP2eikIB/2TYHU5dp86HToOl97GlmISnDqezbCCZjdtVrjnedp8yl/gcSRcOOKoWGJ2k2STV1lMkGHWXD/Z2D2hHPrtEHVpWyM0Ikkm7qu6WgYmACeQXD5W9gUBpcOGB2VqIUk2QhoGK694mDpALnpEPcAnFlrdFSilpFkIzQ+zWHwTmgcqZWN2f4YHHtbBuMS1UaSjfgvdwv0WwdtpqKVjXkZ9j0rZWNEtZBkI4pycYUei6DbQsAEJ5dCwsOQf9noyEQNJ8lGFGcywb3R2jtUrvXg/NfaE8c5PxodmajBJNmI0jUZAYN3gNc9kH3sVtmYXUZHJWooSTaibPW73Cob0w3yLsKWB+HUp0ZHJWogSTaifN4hMDgRmjyqlY3Z9T+Q/EfpqRKVIslGVIxrPbh/Ndz7ojafPBt2T5CyMaLCJNmIinMxQ7e3IWzJrbIxH8HWQXD9otGRiRpAko2ovNZToP8GcLNA5g7Y3Auyvjc6KuHkJNmIu9N4MAzZBfWaQ85J2BwB57caHZVwYpJsxN2ztIfIvdAwAm5chvhIOPmh0VEJJyXJRlSNZ5BWNqbZk6Buwt7JkDQTlM3oyISTkWQjqs7sCb0/ho7/q80ffQt2PAE3rxkbl3AqkmxE9TC5QKc/QsQ/wcUdzqyGr/trQ1YIgc7JJiYmhp49e+Lr60tQUBAjR44kJUWK3ddqLX6tlY3xaACXvrlVNibZ6KiEE9A12Wzbto2oqCj27NlDXFwcN27cYMiQIVy9elXP3QqjBfW9VTamLVw7A5v7wLkNRkclDObQ6gqZmZkEBQWxbds2HnjggXLbS3WFGi7vEux4HC7Ea5dZ3f4CodOMjkpUI6etrpCVlQVAQECAI3crjOIRAP03QsuntN6pA8/D/hfAdtPoyIQBXB21I5vNRnR0NH369KFjx44ltimp1reo4czuEP4P7ZLq21nww9+0cXH6fApuvkZHJxzIYWc2UVFRHD58mBUrVpTaJiYmBovFYp+sVqujwhN6Mpmgw0y4/9+3ysash7j74eoZoyMTDuSQezbTpk3jiy++IDExkRYtWpTarqQzG6vVKvdsapOL+yDxEbh+ATyDod+X0KCH0VGJu+Q092yUUkybNo01a9awdevWMhMNgIeHB35+fkUmUcs0DNNecfC/D66fh68fgDOfGx2VcABdk01UVBQfffQRn3zyCb6+vpw/f57z58+Tm5ur526Fs6vXTBtutPFQKMiF7aPg6P/JYFy1nK6XUSaTqcTlsbGxTJo0qdz1peu7lrPdhAPRcHyRNt/qaej5Hri4GRqWqLjK/EZ17Y1y4CM8oiZycYWe74JfWzj4Wzj5d62nqu8qcK9vdHSimsm7UcJ4oS/8t2zMha1a2ZgrJ42OSlQzSTbCOdwzXCv/690Esr/XRv/L3Gl0VKIaSbIRzqN+Z61sTED328rGfGJ0VKKaSLIRzsWrMQzaBk1Ggi0fdo2D5DnSU1ULSLIRzse1HvRdDe1e1uaTX4dd46HguqFhiaqRZCOck8kFuv4fhH0AJlc4/cmtsjGZRkcm7pIkG+HcWj8DAzbeKhuzU8rG1GCSbITzCx4IQ3ZDvRbaczibe8H5LUZHJSpJko2oGSztbpWN6Q03siB+KJz4u9FRiUqQZCNqDs9AGLgFmo3VysbsewYOvSJlY2oISTaiZrGXjZmtzR/7P9gxWsrG1ACSbETNYzJBp9ch4qNbZWM+h6/7SdkYJyfJRtRcLcbBg1tulY3ZD5vC4JdvjY5KlEKSjajZgu6HIXvBLxSundWGG/1pvdFRiRJIshE1n28rrWu80QC4maMNO5ryN6OjEneQZCNqB/f6WtmYVpNvlY15AfY/L2VjnIgkG1F7mN0hbCl0eUub/+Fd2PYI3JCSQM5Ako2oXUwmaP877UVOsxekb7hVNibN6MjqPEk2onayPqYNVeEZDJeTtZ6qn78xOqo6Tddkk5iYyIgRIwgJCcFkMrF27Vo9dydEUQ163lY25oL2LE7aaqOjqrN0TTZXr16lc+fOLFq0SM/dCFG6ek1vlY15SCsbs+NxOPqWDMZlAF2rKzz00EM89NBDeu5CiPK5+UG//8DBGVqt8aSZkP0D9Fys3VQWDiH3bETd4OIKPf4K3f+qDcz144eQMBTyfzE6sjrDqZJNXl4e2dnZRSYhqlXo8/DAl+DqAxfiYXMEXDlhdFR1glMlm5iYGCwWi32yWq1GhyRqo3sevlU2xgrZKdpgXBk7jI6q1nOqZDNr1iyysrLs05kzZ4wOSdRW9TtpPVUBPSDvZ9g6EFI/MjqqWs2pko2Hhwd+fn5FJiF0U1g2xvqYVjZm96/hu9nSU6UTXZNNTk4OSUlJJCUlAZCamkpSUhJpafI0p3ASrt5w/7+h/Sva/OE/arWqpGxMtTMppV8aT0hIYMCAAcWWT5w4kWXLlpW7fnZ2NhaLhaysLDnLEfo7+Q/Y95w25GjD3vDAWm0oUlGqyvxGdU02VSXJRjjc+a2wfRTcuKxVc+i/XhtsXZSoMr9Rp7pnI4Thgh/UxsbxaQlXU7Wu8fNfGx1VrSDJRog7We7VRv8L7HNb2ZilRkdV40myEaIkng218Y2bjwNVAPumwKGXpWxMFUiyEaI0Zg+I+BfcN0ebP/a2dj/n5lVj46qhJNkIURaTCe57TatV5eIOZ9dqQ1VcO2d0ZDWOJBshKqL5/8DAreDREC4dgM3hUjamkiTZCFFRgX20Vxz87r1VNqYP/LTO6KhqDEk2QlSGT8tbZWMGavduEh+F7/8irzhUgCQbISrL3R8GbIBWT2u9UwejYf80KRtTDkk2QtwNFzcI+wC6/hkwwfH3YNsIKRtTBkk2QtwtkwnavXRb2ZiNsLkPXD1tdGROSZKNEFVl/RUMStSGrMg6DJvC4eJeo6NyOpJshKgODXporzj4d9LKxmzpD2mrjI7KqUiyEaK61LNqZWNChmnj4ewYDUdipKfqFkk2QlQnN1944AsIna7Nf/t72DsZCvKNjcsJSLIRorq5mKH7O9Dj3VtlY2IhPhLyLhkdmaEk2Qihl7ZR0G8duPpCRoI2Nk72caOjMowkGyH0FPIQDNkJ3k3hyg+3ysZsNzoqQ0iyEUJv/vfdKhvTE/Iv3Sob8y+jo3I4hySbRYsW0bx5czw9PQkPD2ffvn2O2K1wIgU2xe6TP/NF0k/sPvkzBbY61kPjFQyDEsD6ONhuwO4J8N1rTtdTpef35FptWyrFypUrmTFjBu+//z7h4eG88847REZGkpKSQlBQkN67F05g4+F05nx5lPSs/5ZHaWzxZPaI9gzt2NjAyBzM1RvuXwnf/gGOvgmH58KV49ArFsyeRken+/eke3WF8PBwevbsybvvvguAzWbDarXy/PPPM3PmzDLXleoKNd/Gw+lM/eggd/4jM936c/H4bnUr4RQ6GasNNapuQsOIW2VjjPvP926/J6eprpCfn8+BAwcYNGjQf3fo4sKgQYPYvXu3nrsWTqDAppjz5dFi/4AB+7I5Xx6te5dUAK2eggc3g5s/XNytveKQddSQUBz1PemabC5evEhBQQGNGjUqsrxRo0acP3++WPu8vDyys7OLTKLm2pd6qcgp+Z0UkJ51nX2pdfT5k0YDIHIP+LSCq6e0rvH0OIeH4ajvyal6o2JiYrBYLPbJarUaHZKogowrFSthW9F2tZJfKAzZA4F9teEpEh6CEx84NARHfU+6JpuGDRtiNpu5cOFCkeUXLlwgODi4WPtZs2aRlZVln86cOaNneEJnQb4Vu+lZ0Xa1lmdDeDAOmo+/VTbmWTj4EtgKHLJ7R31PuiYbd3d3unfvzpYtW+zLbDYbW7ZsISIiolh7Dw8P/Pz8ikyi5gprEUBji6f9JuOdTGi9HWEtAhwZlnMye0DEP+G+P2rz38+HHY4pG+Oo70n3y6gZM2awdOlSli9fzrFjx5g6dSpXr17lqaee0nvXwmBmFxOzR7QHKPYPuXB+9oj2mF1K+2dex5hMcN//Qu9PwcUDzn4BcQ/AtZ903a2jvifdk82YMWN4++23ee211+jSpQtJSUls3Lix2E1jUTsN7diYxeO7EWwpegoebPGsu93e5Wn+JAyMB49A+OWg1lN16ZCuu3TE96T7czZVIc/Z1B4FNsW+1EtkXLlOkK92Si5nNOXISYWEYZB9DFzraWc8TUbousvKfk+V+Y1KshHCmeVf1gbhOv81YIJuC7SxckzOkaid5qE+IUQVuftD/6+g9RRAwcHfwv6oGlk2RpKNEM7OxQ16vg9d56OVjVkM24ZDfpbRkVWKJBshagKTCdrNgL6fg9kb0jdp5X9zThkdWYVJshGiJrGOhMHbwSsEso7A5ppTNkaSjRA1TUA3bTCu+l3geoZWNub0Z0ZHVS5JNkLURN5NYNB2uGeEVjZm5xg4Ms/pBuO6nSQbIWoqNx/ouwZCo7X5b/8Ae55y2rIxkmyEqMlczNB9IfR8D0xmSF0O8UMg72ejIytGko0QtUGbqbeVjdnmlGVjJNkIUVuEDIUhu6BeM21s48294MI2o6Oyk2QjRG3i3xGG7IUG4VrZmPjB8OM/jY4KkGQjRO3j1Uh7a7zpaK1szJ6J8O2roGyGhiXJRojayNUL+qyADn/Q5o+8ATvHws1cw0KSZCNEbWVygc5/gl7LtPer0j6DLQMg90K5q+pBko0QtV3LiTAgDtzrw897tVccLh9xeBiSbISoCxr106o4+LSGq6chrjekb3ZoCJJshKgr/NpqdaqCHrhVNuZhOP6+w3YvyUaIusSjAQzYDC0mamVjvpkKB2Y4pGyMbsnmjTfeoHfv3nh7e+Pv76/XboQQlWX2gF6x0OlP2nzKQtj+GNzI0XW3uiWb/Px8Ro8ezdSpU/XahRDibplM0PEP0GelVjbmp//A133h2lnddumq14bnzJkDwLJly/TahRCiqpo9AfWaQuKj8EuSVjam3zoI6Frtu5J7NkLUdQ17aa84WNpD7jmIux/O/qfad+NUySYvL4/s7OwikxDCAXyaw+BdEDwECq5B4khIW1Wtu6hUspk5cyYmk6nM6fvvv7/rYGJiYrBYLPbJarXe9baEEJXkboH+66H1c1o3efDAat18pYrUZWZm8vPPZQ/K07JlS9zd3e3zy5YtIzo6msuXL5e7/by8PPLy8uzz2dnZWK1WKVInhCMpBTeytJpV5ahMkbpK3SAODAwkMDCwMqtUioeHBx4eHrptXwhRASZThRJNZenWG5WWlsalS5dIS0ujoKCApKQkAFq3bo2Pj49euxVCOCndks1rr73G8uXL7fNdu2pdafHx8fTv31+v3QohnFSl7tk4WmWuB4UQjleZ36hTdX0LIWovSTZCCIeQZCOEcAjdbhBXh8LbSfIksRDOqfC3WZFbv06dbK5cuQIgTxIL4eSuXLmCxWIps41T90bZbDbOnTuHr68vJpOpzLaFTxufOXOm1vRc1bZjqm3HA3JMSimuXLlCSEgILi5l35Vx6jMbFxcXmjRpUql1/Pz8as2XXqi2HVNtOx6o28dU3hlNIblBLIRwCEk2QgiHqDXJxsPDg9mzZ9eqFzlr2zHVtuMBOabKcOobxEKI2qPWnNkIIZybJBshhENIshFCOIQkGyGEQ9SoZLNo0SKaN2+Op6cn4eHh7Nu3r8z2//73v7n33nvx9PTkvvvu46uvvnJQpBVXmWNatmxZsQHmPT09HRht2RITExkxYgQhISGYTCbWrl1b7joJCQl069YNDw8PWrdu7XR1xip7TAkJCSUWAjh//rxjAi5HTEwMPXv2xNfXl6CgIEaOHElKSkq561XHb6nGJJuVK1cyY8YMZs+ezcGDB+ncuTORkZFkZGSU2H7Xrl2MHTuWyZMnc+jQIUaOHMnIkSM5fPiwgyMvXWWPCbSnOtPT0+3T6dOnHRhx2a5evUrnzp1ZtGhRhdqnpqYybNgwBgwYQFJSEtHR0Tz99NNs2rRJ50grrrLHVCglJaXI9xQUFKRThJWzbds2oqKi2LNnD3Fxcdy4cYMhQ4Zw9erVUteptt+SqiHCwsJUVFSUfb6goECFhISomJiYEts/8cQTatiwYUWWhYeHq2effVbXOCujsscUGxurLBaLg6KrGkCtWbOmzDa/+93vVIcOHYosGzNmjIqMjNQxsrtXkWOKj49XgPrll18cElNVZWRkKEBt27at1DbV9VuqEWc2+fn5HDhwgEGDBtmXubi4MGjQIHbv3l3iOrt37y7SHiAyMrLU9o52N8cEkJOTQ7NmzbBarTz66KMcOXLEEeHqwtm/o6ro0qULjRs3ZvDgwezcudPocEqVlZUFQEBAQKltqut7qhHJ5uLFixQUFNCoUaMiyxs1alTqtfD58+cr1d7R7uaYQkND+fDDD/niiy/46KOPsNls9O7dm7Nn9SsGr6fSvqPs7Gxyc3MNiqpqGjduzPvvv8/q1atZvXo1VquV/v37c/DgQaNDK8ZmsxEdHU2fPn3o2LFjqe2q67fk1G99i6IiIiKIiIiwz/fu3Zt27dqxZMkS5s6da2BkolBoaCihoaH2+d69e3Py5EkWLlzIv/71LwMjKy4qKorDhw+zY8cOh+yvRpzZNGzYELPZzIULF4osv3DhAsHBwSWuExwcXKn2jnY3x3QnNzc3unbtyokTJ/QIUXelfUd+fn54eXkZFFX1CwsLc7rvaNq0aaxbt474+Phyh3Gprt9SjUg27u7udO/enS1bttiX2Ww2tmzZUuR/+ttFREQUaQ8QFxdXantHu5tjulNBQQHJyck0btxYrzB15ezfUXVJSkpymu9IKcW0adNYs2YNW7dupUWLFuWuU23f093cwTbCihUrlIeHh1q2bJk6evSomjJlivL391fnz59XSin161//Ws2cOdPefufOncrV1VW9/fbb6tixY2r27NnKzc1NJScnG3UIxVT2mObMmaM2bdqkTp48qQ4cOKCefPJJ5enpqY4cOWLUIRRx5coVdejQIXXo0CEFqAULFqhDhw6p06dPK6WUmjlzpvr1r39tb//jjz8qb29v9fLLL6tjx46pRYsWKbPZrDZu3GjUIRRT2WNauHChWrt2rTp+/LhKTk5W06dPVy4uLurrr7826hCKmDp1qrJYLCohIUGlp6fbp2vXrtnb6PVbqjHJRiml/va3v6mmTZsqd3d3FRYWpvbs2WP/rF+/fmrixIlF2n/22Weqbdu2yt3dXXXo0EGtX7/ewRGXrzLHFB0dbW/bqFEj9fDDD6uDBw8aEHXJCrt975wKj2HixImqX79+xdbp0qWLcnd3Vy1btlSxsbEOj7sslT2mt956S7Vq1Up5enqqgIAA1b9/f7V161Zjgi9BSccCFPl71+u3JENMCCEcokbcsxFC1HySbIQQDiHJRgjhEJJshBAOIclGCOEQkmyEEA4hyUYI4RCSbIQQDiHJRgjhEJJshBAOIclGCOEQkmyEEA7x/wGnjYxdyFA8bwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = np.array([[0,1],\n",
    "              [1,1],\n",
    "              [2,1]])\n",
    "b = np.array([6,0,0])\n",
    "\n",
    "# A has linearly independent columns so compute x hat using the following reformulation\n",
    "# of the normal equations:  \n",
    "x_hat = np.linalg.inv(A.T @ A) @ A.T @ b\n",
    "print(x_hat)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.scatter([0, 1, 2], [6, 0, 0])\n",
    "plt.plot([i for i in range(0, 3)], [x_hat[0]*i + x_hat[1] for i in range(0, 3)], \n",
    "         label=\"best fit line\", c=\"orange\")\n",
    "plt.title(f\"y = {x_hat[0] :.1f}x + {x_hat[1] :.1f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the least-squares solution minimizes the residual sum of squares, or the sum of squared entries of vector $b - A\\hat{X}$.  \n",
    "In this scenario, that corresponds graphically to minimizing the vertical distance between each point and the line of best fit.  \n",
    "\n",
    "## The General Linear Model\n",
    "_Linear Algerba and its Applications, pg. 373._  \n",
    "\n",
    "For some applications, we may want to fit our data points with something other than a straight line.  \n",
    "In general, we still want to solve the matrix equation $X\\beta = y$ while minimizing the sum of the squared residuals. Statisticians write the residual vector as $\\epsilon = y - X\\beta$, and thus express the outcome variable as $y = X\\beta + \\epsilon$.\n",
    "- Any equation of this form is a \"linear model\".  \n",
    "\n",
    "When fitting general curves, we want to find a function $y = f(x) = \\beta_1g_1(x) + \\beta_2g_2(x) + ... + \\beta_mg_m(x)$ that best approximates our data (x,y), where functions $g_m$ are fixed, known functions of $x$ and the $\\beta$ are parameters to be solved for. The $g_m(x)$ can be any kind of function, since once they are evaluated they become scalars and we can find the least-squares solution that minimizes the sum of the squares of the vertical distances between the graph of $y = f(x)$ and our original data points. As noted in [Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/least-squares.html), the method of least squares was invented by Gauss to find a best-fit ellipse.  \n",
    "Therefore, we will still be solving the matrix equation $X\\beta = y$, but the specific form of $X$ will be changing.  \n",
    "- After transforming $X$, we still solve our normal equations $X^TX\\beta = X^Ty$ for $\\beta$  \n",
    "- And if $X$ is invertible, we can always use $\\hat{\\beta} = (X^TX)^{-1}X^Ty$  \n",
    "    - Recall that $X$ will be invertible if its columns are orthogonal, and we have tricks that can help us produce orthogonal columns (Gram-Schmidt for example).  \n",
    "\n",
    "Some examples:  \n",
    "We could fit a polynomial function of x to be as close to the data as possible by using an equation of the form $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$, and thus solving the system of equations:  \n",
    "$y_1 = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2 + \\epsilon_1$  \n",
    "$y_2 = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2 + \\epsilon_2$    \n",
    "...  \n",
    "$y_n = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2 + \\epsilon_n$,  \n",
    "- We can write this in matrix notation as $y = X \\beta + \\epsilon$  \n",
    "- $X$ is the matrix whose columns are [$1, x, x^2$]   \n",
    "\n",
    "Another common scenario is \"multiple regression\", in which we want to model an outcome variable as a function of multiple inputs.  \n",
    "We could use the simple equation $y = \\beta_0 + \\beta_1 u + \\beta_2 v$ or a more general equation $y = \\beta_0 + \\beta_1 u + \\beta_2 v + \\beta_3 u^2 + \\beta_4 uv + \\beta_5 v^2$.  \n",
    "We can write this as $y = X\\beta + \\epsilon$ where $X$ has the columns [$1, u, v$] in the simple case and [$1, u, v, u^2, uv, v^2$] in the other.  \n",
    "- Note that, for the simple case, we can imagine a graph in $\\R^3$ with $u$ and $v$ forming a horizontal 2-D plane and $y$ being the vertical axis. Each point displays $y$ as a function of $u$ and $v$ - ie, $y = f(u,v)$. Our least-squares approximation of this data is a least-squares plane that minimizes the vertical distances from the oberved $y$ to the plane.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code examples\n",
    "\n",
    "Calculating orthogonal projections of $y$ onto $Col(X)$, and calculating $\\beta$ for the linear model $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: fitted values produced by the hat matrix\n",
      "\t[0.09719569 0.40254417 0.03624774 0.47811664 0.42241259]\n",
      "betas: the fitted parameters of the model\n",
      "\t[-0.06731001  0.59008736]\n",
      "X @ betas: fitted values produced by inputting X into the fit model\n",
      "\t[0.09719569 0.40254417 0.03624774 0.47811664 0.42241259]\n"
     ]
    }
   ],
   "source": [
    "# Generate some random data - sample size of 5, with 2 input features and a target y\n",
    "X = np.random.rand(5,2)\n",
    "y = np.random.rand(5)\n",
    "\n",
    "# compose some of the transformations, not quite H though\n",
    "M = np.linalg.inv(X.T @ X) @ X.T\n",
    "# one more matrix multiplication makes this the hat / projection matrix\n",
    "H = X @ M\n",
    "\n",
    "# compute y_hat - orthogonally project y onto the column space of X\n",
    "y_hat = H @ y\n",
    "# calculate the betas / coefficients\n",
    "betas = M @ y\n",
    "\n",
    "print(f\"y_hat: fitted values produced by the hat matrix\\n\\t{y_hat}\")\n",
    "print(f\"betas: the fitted parameters of the model\\n\\t{betas}\")\n",
    "print(f\"X @ betas: fitted values produced by inputting X into the fit model\\n\\t{X @ betas}\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some real data, and comparing manual results with a popular statistics/machine learning package, sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually computed betas:\n",
      "\t[ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n",
      "  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218]\n",
      "Skearn computed betas:\n",
      "\t[ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n",
      "  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218]\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "\n",
    "# now with some real data\n",
    "X = diabetes[\"data\"]\n",
    "y = diabetes[\"target\"]\n",
    "betas = (np.linalg.inv(X.T @ X) @ X.T) @ y\n",
    "print(f\"Manually computed betas:\\n\\t{betas}\")\n",
    "\n",
    "m = linear_model.LinearRegression()\n",
    "m.fit(X, y)\n",
    "m.coef_\n",
    "print(f\"Skearn computed betas:\\n\\t{m.coef_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "future reading:  \n",
    "- https://socialsciences.mcmaster.ca/jfox/Books/Companion/appendices/Appendix-Robust-Regression.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
