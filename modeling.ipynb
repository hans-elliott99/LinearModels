{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "from sklearn import linear_model\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "## Overview\n",
    "In general, a linear model takes the form $y = X\\beta + \\epsilon$, where $\\epsilon$ is the \"residual\" (the difference between the true value of $y$ and $X\\beta = y_{Col(X)} = \\hat{y}$)  \n",
    "\n",
    "This can also be expressed without matrices as:  \n",
    "$f(X) = \\beta_{0} + \\sum_{j=1}^{p} X_j \\beta_{j}$  \n",
    "Such a model is appropriate when the regression function, $E(Y|X)$, is linear or when a linear model is a reasonable approximation.  \n",
    "- Recall that, no matter how we transform the input variables that make up the columns of $X$, this model is linear since it is linear in its parameters.  \n",
    "\n",
    "Such a mdoel is typically fit using least-squares, where we pick some coefficients, $\\beta$, to minimize the residual sum of squares:  \n",
    "$RSS(\\beta) = \\sum_{i=1}^{N} (y_i - f(x_i))^2$  \n",
    "which measures the average lack of fit.  \n",
    "\n",
    "## Procedure\n",
    "We seek the linear function of X that best approximates the data. In other words, we want to fit a hyperplane that is as close as possible to all of the individual data points. We do this by minimizing the RSS.  \n",
    "- This is a convenient method because we can achieve exactly this using least-squares.  \n",
    "- Statistically, minimizing the RSS is reasonable if the observations ($x_i, y_i$), represent independent random draws from their population.    \n",
    "\n",
    "Let $X$ be the $n \\times (p + 1)$ matrix, each row an input vector with a 1 in the first position, for the constant. \n",
    "- In practice, X is our training data, with columns corresponding to features / predictors / independent variables, and each row a different sample or observation.  \n",
    "\n",
    "Let $y$ be the $n$-length vector of outputs.  \n",
    "- In practice, y is our target variable or dependent variable  \n",
    "\n",
    "Since finding the coefficients $\\beta$ to the linear model $f(X)$ is equivalent to finding the least-squares solution to matrix equation $X\\beta = y$, we just need to solve the normal equations:  \n",
    "$X^TX\\beta = X^Ty$  \n",
    "\n",
    "\n",
    "Review of the basic linear algerba facts that makes this work:  \n",
    "- To model $y$ as a linear function of $X$, we want to solve the matrix equation $X\\beta = y$ for the unkown vector $\\beta$. However, this is an inconsistent equation - there is no exact solution for $\\beta$ - and thus we want the best-approximate solution, which we define as the solution that solves $Ax$ as colsely as possible, minimizing the RSS.  \n",
    "- The closest vector of the form $X\\beta$ to $y$ is a known object in linear algebra - it is the **orthogonal projection of $y$ onto the column space of $X$**. Thus, we can solve the **consistent** matrix equation $X\\beta = y_{Col(X)}$ for the $\\beta$ vector which minimizes the sum of the squared differences between $y$ and $y_{Col(X)}$.  \n",
    "- It turns out that solving for $\\beta$ arises from the computations used to orthogonally project $y$ onto $Col(X)$. Specifically, since $y_{Col(X)} = X\\beta$, we solve for $\\beta$ by solving the **normal equations**:  \n",
    "$X^TX\\beta = X^Ty$  \n",
    "- This can be solved via row-reduction, or if the columns of $X$ are linearly indepdent and thus $X$ is invertible, we can solve $\\beta = (X^TX)^{-1}X^Ty$\n",
    "    - Recall that if the columns of $X$ are not linearly independent (often, this is called collinearity in statistics) then there are infinite least-squares solutions (although $\\hat{y} = X\\hat{\\beta}$ are still the orthogonal projection of $y$ onto $Col(X)$). Usually, this is solved by dropping one of the collinear variables.  \n",
    "\n",
    "Once we have computed $\\hat{\\beta}$, we can calculate $y_{Col(X)}$, which we will call $\\hat{y}$. We put a hat on the coefficient vector, $\\hat{\\beta}$, to denote that these are estimated coefficients for some \"true\", underlying model. We put the hat on $\\hat{y}$ because it is the orthogonal projection of $y$, and thus is also an estimate of the true outcome (specifically, the closest estimate of the form $X\\hat{\\beta}$).  \n",
    "\n",
    "$\\hat{y} = X\\hat{\\beta} = X(X^TX)^{-1}X^Ty$\n",
    "- Recall that $X(X^TX)^{-1}X^T$ is called the projection matrix since it projects $y$ onto the column space of $X$ and the hat matrix because it puts the hat on $y$.  \n",
    "- Intuitively, this means that we want to calculate coefficients that reproduce $y$ as closely as possible when given inputs $X$. Geometrically, we learned that we can think of the column space of $X$ as its own coordinate system, with each column vector defining an axis. The coefficients, $\\hat{\\beta}$, provide the coordinates to reach $\\hat{y}$ in this coordinate system.  \n",
    "    - This has implications for when we feed new, \"test\" data into the model. Our coefficients are fit based on a training sample for which we have both x's and y's, but at some point we may hope to use these coefficients to predict y's for new x's. Thus we hope these \"coordinates\" generalize well-enough that they map the new inputs, $X_{new}$ to the correct $y_{Col(X_{new})}$. Say the training sample was a good representation of the total population, and therefore we have computed coefficients that are close to the \"true\" underlying model. We thus have minimal bias, or error in the definition of our model. The new sample's inputs are those similair to the original sample, so the new \"coordinate system\" (column space of $X_{new}$) is similair to the original coordinate system (column space of $X$), and thus our \"coordinates\" (the coefficients, $\\beta$) will behave well. Now imagine that our training sample was not a good representation of the overall population, so that $X_{new}$ varies significantly from $X$. Then we would expect our computed \"coordinates\" to perform poorly when used in this new \"coordinate system\".  \n",
    "    - Imagine you pick up a map and calculate the coordinates from some central point to your destination. You write down the coordinates but lose the map. Now you are out on the trail and don't know where to go, but you find a map in a puddle on the ground. You pick up the soggy, runny, distored map and use the coordinates you wrote down to trace out the location of your destination. If the map is badly distorted, your coordinates are going to lead you somewhere totally different than you intended. If it turns our there was little distortion, or the distortion maps that of your own map, then your coordinates will be very useful in predicting where to go.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "Up until now, the only statistical assumptions we have made are: \n",
    " - The $E(Y | X)$ is linear, or a linear model is a reasonable approximation.  \n",
    " - The training observations ($x_i, y_i$) represent independent random draws from the population.  \n",
    "\n",
    "We now assume:  \n",
    "- The observations, $y_i$, are uncorrelated and have constant variance $\\sigma^2$  \n",
    "- That the $x_i$ are fixed, not random.  \n",
    "\n",
    "Then the variance-covariance matrix of the least-squares parameter estimates is derived to be:    \n",
    "$Var(\\hat{\\beta}) = (X^TX)^{-1}\\sigma^2$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
